theme_set(theme_bw())  # pre-set the bw theme.
g <- ggplot(gapminder, aes(gdpPercap, lifeExp, size = pop, frame = year)) +
geom_point() +
geom_smooth(aes(group = year),
method = "lm",
show.legend = FALSE) +
facet_wrap(~continent, scales = "free") +
scale_x_log10()  # convert to log scale
gganimate(g, interval=0.2)
library(NbClust)
install.packages("NbClust")
# Preparing the data
# load data from csv
BlackFriday <- read.csv("BlackFriday.csv",stringsAsFactors = FALSE, header = TRUE, na.strings = c ('NA',''))
setwd("~/Desktop/Data Science/BigData /Project2BigData")
# Preparing the data
# load data from csv
BlackFriday <- read.csv("BlackFriday.csv",stringsAsFactors = FALSE, header = TRUE, na.strings = c ('NA',''))
# Load libraries
library(readxl)
library(dplyr)
library(psych)
library(ggplot2)
library(base)
library(data.table)
library(plyr)
library(stringr)
library(cluster)
library(class)
library(outliers)
library(NbClust)
# Preparing the data
# load data from csv
BlackFriday <- read.csv("BlackFriday.csv",stringsAsFactors = FALSE, header = TRUE, na.strings = c ('NA',''))
# Set names for data
names(BlackFriday) <- c("User_ID","Product_ID","Gender","Age","Occupation","City_Category","Stay_In_Current_City_Years","Marital_Status","Product_Category_1","Product_Category_2","Product_Category_3","Purchase")
# Check out the data
BlackFriday
# Check out the data
BlackFriday
# Copy data to a working variable
BlackFridayClean <- BlackFriday
# Removing missing data
BlackFridayClean <- na.omit(BlackFridayClean)
# View BlackFridayClean
BlackFridayClean
colnames(BlackFridayClean)
bf <- BlackFridayClean
colnames(bf)
# Summary
summary(bf)
# Describe
describe(bf)
bf
# Make gender numeric
# 0 = female, male = 1
bf[bf=="F"] <- 0
bf[bf=="M"] <- 1
bf
# Change gender to numeric
bf$Gender <- as.numeric(as.character(bf$Gender))
bf$Age
str(bf)
# Copy bf to modify it
bfCopy <- bf
# Check unique values of Age
unique(bfCopy$Age)
# Get number of rows in data frame
print(nrow(bfCopy))
# Set age range to random values
for(row in 1:nrow(bfCopy)){
# Assuming the youngest age people shop during BlackFriday is 1
if(bfCopy[row,"Age"]=="0-17"){ # Generation Z
bfCopy[row,"Age"] <- floor(runif(1,min=13,max=17))
} else if(bfCopy[row,"Age"]=="18-25"){ # Millenials
bfCopy[row,"Age"] <- floor(runif(1,min=18,max=25))
} else if(bfCopy[row,"Age"]=="26-35"){ # Millenials
bfCopy[row,"Age"] <- floor(runif(1,min=26,max=35))
} else if(bfCopy[row,"Age"]=="36-45"){ # Generation X
bfCopy[row,"Age"] <- floor(runif(1,min=36,max=45))
} else if(bfCopy[row,"Age"]=="46-50"){ # Generation X
bfCopy[row,"Age"] <- floor(runif(1,min=46,max=50))
} else if(bfCopy[row,"Age"]=="51-55"){ # Baby Boomers
bfCopy[row,"Age"] <- floor(runif(1,min=51,max=55))
} else if(bfCopy[row,"Age"]=="55+"){ # Baby Boomers
bfCopy[row,"Age"] <- floor(runif(1,min=56,max=80))
}
print(row)
}
library(VIM)
install.packages("VIM")
library(VIM)
# Plots the amount of missing values in each column
aggr(BlackFriday)
bf <- bfCopy
bf
str(bf)
# Check unique ages
unique(bf$Age)
# Structure of bf
str(bf)
# Change age to numeric
bf$Age <- as.numeric(as.character(bf$Age))
bf$Age
# Getting rid of first character of Product_ID
for(row in 1:nrow(bf)){
bf[row,"Product_ID"] <- substring(bf[row,"Product_ID"],2,nchar(bf[row,"Product_ID"]))
print(row)
}
bf$Product_ID
# Change Product_ID to numeric
bf$Product_ID <- as.numeric(as.character(bf$Product_ID))
bf$Product_ID
# Change city_category to numeric, A = 1, B = 2, C = 3
unique(bf$City_Category)
bf$City_Category
bf$City_Category[bf$City_Category=="A"] <- 1
bf$City_Category[bf$City_Category=="B"] <- 2
bf$City_Category[bf$City_Category=="C"] <- 3
bf$City_Category <- as.numeric(as.character(bf$City_Category))
# Change Stay_in_current_city_years to numeric
unique(bf$Stay_In_Current_City_Years)
# Change 4+ to random number between 4 and 80 (oldest person is 80)
for(row in 1:nrow(bf)){
if(bf[row,"Stay_In_Current_City_Years"]=="4+"){
bf[row,"Stay_In_Current_City_Years"] <- floor(runif(1,min=4,max=80))
}
print(row)
}
bf$Stay_In_Current_City_Years
unique(bf$Stay_In_Current_City_Years)
bf$Stay_In_Current_City_Years <- as.numeric(as.character(bf$Stay_In_Current_City_Years))
# Reduce data set
colnames(bf)
# Save bf
bfOriginal <- bf
# Add column for id to identify when creating training and testing sets
bf$id <- 1:nrow(bf)
colnames(bf)
# Remove product id column
bf <- within(bf,rm(Product_ID))
# Remove product categories
bf <- within(bf,rm(Product_Category_1))
bf <- within(bf,rm(Product_Category_2))
bf <- within(bf,rm(Product_Category_3))
# Remove city
bf <- within(bf,rm(City_Category))
colnames(bf)
# For k-means clustering, the division into training and testing
# sets is done in order to identify the number of clusters aka k
# 50-50
train50 <- bf %>% dplyr::sample_frac(.50)
test50  <- dplyr::anti_join(bf, train50, by = 'id')
train50 <- na.omit(train50)
test50 <- na.omit(test50)
train50 <- within(train50,rm(id))
test50 <- within(test50,rm(id))
# 60-40
train60 <- bf %>% dplyr::sample_frac(.60)
test40  <- dplyr::anti_join(bf, train60, by = 'id')
train60 <- na.omit(train60)
test40 <- na.omit(test40)
train60 <- within(train60,rm(id))
test40 <- within(test40,rm(id))
# 70-30
train70 <- bf %>% dplyr::sample_frac(.70)
test30  <- dplyr::anti_join(bf, train70, by = 'id')
train70 <- na.omit(train70)
test30 <- na.omit(test30)
train70 <- within(train70,rm(id))
test30 <- within(test30,rm(id))
# Cluster Analysis
#  Check which values are numeric
sapply(bf,is.numeric) # User_ID, Occupation, Product Category 1, Product Category 2, Product Category 3, Purchase, Marital Status are numeric
# Try clustering on entire data set
bfn <- bf
# Entire data set
bfn.scaled <- scale(bfn)
# Scale the data
train50.scaled <- scale(train50)
train60.scaled <- scale(train60)
train70.scaled <- scale(train70)
# Min-Max Normalization
normalize <- function(x) {(((x-min(x))/max(x)-min(x)))}
# Apply normalize function
normalize(bfn.scaled)
normalize(train50.scaled)
normalize(train60.scaled)
normalize(train70.scaled)
# Stay_In_Current_City_Years
out_1 <- outlier(bfn.scaled[, "Stay_In_Current_City_Years"], opposite = FALSE, logical=FALSE)
out50_1 <- outlier(train50.scaled[, "Stay_In_Current_City_Years"], opposite = FALSE, logical=FALSE)
out60_1 <- outlier(train60.scaled[, "Stay_In_Current_City_Years"], opposite = FALSE, logical=FALSE)
out70_1 <- outlier(train60.scaled[, "Stay_In_Current_City_Years"], opposite = FALSE, logical=FALSE)
out50_1
out60_1
out70_1
rm.outlier(bfn.scaled[,"Stay_In_Current_City_Years"],fill=FALSE,median=FALSE,opposite=FALSE)
rm.outlier(train50.scaled[, "Stay_In_Current_City_Years"], fill = FALSE, median = FALSE, opposite = FALSE)
rm.outlier(train60.scaled[, "Stay_In_Current_City_Years"], fill = FALSE, median = FALSE, opposite = FALSE)
rm.outlier(train70.scaled[, "Stay_In_Current_City_Years"], fill = FALSE, median = FALSE, opposite = FALSE)
# Purchase
out_2 <- outlier(bfn.scaled[,"Purchase"],opposite=FALSE, logical = FALSE)
out50_2<-outlier(train50.scaled[, "Purchase"], opposite = FALSE, logical=FALSE)
out60_2<-outlier(train60.scaled[, "Purchase"], opposite = FALSE, logical=FALSE)
out70_2<-outlier(train70.scaled[, "Purchase"], opposite = FALSE, logical=FALSE)
rm.outlier(bfn.scaled[,"Purchase"],fill=FALSE,median=FALSE,opposite=FALSE)
rm.outlier(train50.scaled[, "Purchase"], fill = FALSE, median = FALSE, opposite = FALSE)
rm.outlier(train60.scaled[, "Purchase"], fill = FALSE, median = FALSE, opposite = FALSE)
rm.outlier(train70.scaled[, "Purchase"], fill = FALSE, median = FALSE, opposite = FALSE)
# prints what was removed
out_2
out50_2
out60_2
out70_2
#  Calculate Distances
mean(dist(bfn.scaled[sample(nrow(bfn.scaled),100),]))
mean(dist(train50.scaled[sample(nrow(train50.scaled),100),]))
mean(dist(train60.scaled[sample(nrow(train60.scaled),100),]))
mean(dist(train70.scaled[sample(nrow(train70.scaled),100),]))
#  Select a Clustering Algorithm
# K means on training set
# Create classification labels for training
# k-means clustering of size 3, 5 and 7
bfn.scaled.k3 <- kmeans(bfn.scaled,3)
train50.scaled.k3 <- kmeans(train50.scaled,3)
train60.scaled.k3 <- kmeans(train60,3)
train70.scaled.k3 <- kmeans(train70,3)
bfn.scaled.k5 <- kmeans(bfn.scaled,5)
train50.scaled.k5<- kmeans(train50,5)
train60.scaled.k5<- kmeans(train60,5)
train70.scaled.k5<- kmeans(train70,5)
bfn.scaled.k7 <- kmeans(bfn.scaled,7)
train50.scaled.k7<- kmeans(train50,7)
train60.scaled.k7<- kmeans(train60,7)
train70.scaled.k7<- kmeans(train70,7)
bfn.scaled.k3
train50.scaled.k3
train60.scaled.k3
train70.scaled.k3
bfn.scaled.k5
train50.scaled.k5
train60.scaled.k5
train70.scaled.k5
bfn.scaled.k7
train50.scaled.k7
train60.scaled.k7
train70.scaled.k7
# totts gets bigger as it goes from 50-70
bfn.scaled.k3$totss
train50.scaled.k3$totss
train60.scaled.k3$totss
train70.scaled.k3$totss
bfn.scaled.k5$totss
train50.scaled.k5$totss
train60.scaled.k5$totss
train70.scaled.k5$totss
bfn.scaled.k7$totss
train50.scaled.k7totss
train60.scaled.k7$totss
train70.scaled.k7$totss
# tot.withinns varies, needs to be interpreted to find right number of clusters
bfn.scaled.k3$tot.withinss
train50.scaled.k3$tot.withinss
train60.scaled.k3$tot.withinss
train70.scaled.k3$tot.withinss
bfn.scaled.k5$tot.withinss
train50.scaled.k5$tot.withinss
train60.scaled.k5$tot.withinss
train70.scaled.k5$tot.withinss
bfn.scaled.k7$tot.withinss
train50.scaled.k7$tot.withinss
train60.scaled.k7$tot.withinss
train70.scaled.k7$tot.withinss
# iClust
# Whole data set
iclust(bfn.scaled,nclusters=3)
train50.scaled.k3$cluster
train50.scaled.k3
train50.scaled.k3$cluster
test50.scaled <- scale(test50)
test40.scaled <- scale(test40)
test30.scaled <- scale(test30)
normalize(test50.scaled)
normalize(test40.scaled)
normalize(test30.scaled)
normalize(test50.scaled)
normalize(test40.scaled)
normalize(test30.scaled)
mean(dist(test50.scaled[sample(nrow(test50.scaled),100),]))
#  Calculate Distances
mean(dist(bfn.scaled[sample(nrow(bfn.scaled),100),]))
mean(dist(train50.scaled[sample(nrow(train50.scaled),100),]))
mean(dist(train60.scaled[sample(nrow(train60.scaled),100),]))
mean(dist(train70.scaled[sample(nrow(train70.scaled),100),]))
mean(dist(test50.scaled[sample(nrow(test50.scaled),100),]))
mean(dist(test40.scaled[sample(nrow(test40.scaled),100),]))
mean(dist(test30.scaled[sample(nrow(test30.scaled),100),]))
# run kmeans against the test set to see what clusters it
# assigns then compare with what knn produced
# Knn predicts what the cluster labels for autoclean.test
# should be given training set and classification labels
bf5050.knn3 <- knn(train50.scaled,test50.scaled,train50.scaled.k3$cluster,k=3)
bf6040.knn3 <- knn(train60.scaled,test40.scaled,train60.scaled.k3$cluster,k=3)
bf7030.knn3 <- knn(train70.scaled,test30.scaled,train70.scaled.k3$cluster,k=3)
bf5050.knn5 <- knn(train50.scaled,test50.scaled,train50.scaled.k3$cluster,k=5)
bf6040.knn5 <- knn(train60.scaled,test40.scaled,train60.scaled.k3$cluster,k=5)
bf7030.knn5 <- knn(train70.scaled,test30.scaled,train70.scaled.k3$cluster,k=5)
bf5050.knn7 <- knn(train50.scaled,test50.scaled,train50.scaled.k3$cluster,k=7)
bf6040.knn7 <- knn(train50.scaled,test50.scaled,train60.scaled.k3$cluster,k=7)
bf7030.knn7 <- knn(train50.scaled,test50.scaled,train70.scaled.k3$cluster,k=7)
bf6040.knn7 <- knn(train60.scaled,test40.scaled,train60.scaled.k3$cluster,k=7)
bf7030.knn7 <- knn(train70.scaled,test40.scaled,train70.scaled.k3$cluster,k=7)
library(gmodels)
library(gmodels)
# Use Cross-Tabulation in gmodels package to evaluate the result
# 5050 train/test set
CrossTable(x=test50.scaled.k3$cluster,y=bf5050.kn3,prop.chisq=FALSE)
# Compare K Nearest neighbor against test set
# Apply kmeans to each test set to get classification labels
test50.scaled.k3<- kmeans(test50,3)
test50.scaled.k3$cluster
test40.scaled.k3<- kmeans(test40,3)
test40.scaled.k3$cluster
test30.scaled.k3<- kmeans(test30,3)
test30.scaled.k3$cluster
test50.scaled.k5<- kmeans(test50,5)
test50.scaled.k5$cluster
test40.scaled.k5<- kmeans(test40,5)
test40.scaled.k5$cluster
test30.scaled.k5<- kmeans(test30,5)
test30.scaled.k5$cluster
test50.scaled.k7<- kmeans(test50,7)
test50.scaled.k7$cluster
test40.scaled.k7<- kmeans(test40,7)
test40.scaled.k7$cluster
test30.scaled.k7<- kmeans(test30,7)
test30.scaled.k7$cluster
# Evaluating KNN
install.packages("gmodels")
install.packages("gmodels")
library(gmodels)
# Evaluating KNN
# Try different initial estimates of k, then use Cross tables to
# determine if you are improving or not
# Build a table of false negatives and false positives for each value of k
# Draw a histogram of this table when done
# Use Cross-Tabulation in gmodels package to evaluate the result
# 5050 train/test set
CrossTable(x=test50.scaled.k3$cluster,y=bf5050.kn3,prop.chisq=FALSE)
# Evaluating KNN
# Try different initial estimates of k, then use Cross tables to
# determine if you are improving or not
# Build a table of false negatives and false positives for each value of k
# Draw a histogram of this table when done
# Use Cross-Tabulation in gmodels package to evaluate the result
# 5050 train/test set
CrossTable(x=test50.scaled.k3$cluster,y=bf5050.knn3,prop.chisq=FALSE)
# Evaluating KNN
# Try different initial estimates of k, then use Cross tables to
# determine if you are improving or not
# Build a table of false negatives and false positives for each value of k
# Draw a histogram of this table when done
# Use Cross-Tabulation in gmodels package to evaluate the result
# 5050 train/test set
CrossTable(x=test50.scaled.k3$cluster,y=bf5050.knn3,prop.chisq=FALSE)
CrossTable(x=test50.scaled.k3$cluster,y=bf5050.knn5,prop.chisq=FALSE)
CrossTable(x=test50.scaled.k3$cluster,y=bf5050.knn7,prop.chisq=FALSE)
# 6040 train/test set
CrossTable(x=test40.scaled.k3$cluster,y=bf6040.knn3,prop.chisq=FALSE)
CrossTable(x=test40.scaled.k3$cluster,y=bf6040.knn5,prop.chisq=FALSE)
CrossTable(x=test40.scaled.k3$cluster,y=bf6040.knn7,prop.chisq=FALSE)
# 6040 train/test set
CrossTable(x=test40.scaled.k3$cluster,y=bf6040.knn3,prop.chisq=FALSE)
CrossTable(x=test40.scaled.k3$cluster,y=bf6040.knn5,prop.chisq=FALSE)
CrossTable(x=test40.scaled.k3$cluster,y=bf6040.knn7,prop.chisq=FALSE)
# 7030 train/test set
CrossTable(x=test30.scaled.k3$cluster,y=bf7030.knn3,prop.chisq=FALSE)
CrossTable(x=test30.scaled.k3$cluster,y=bf7030.knn5,prop.chisq=FALSE)
CrossTable(x=test30.scaled.k3$cluster,y=bf7030.knn7,prop.chisq=FALSE)
# Evaluating KNN
# Try different initial estimates of k, then use Cross tables to
# determine if you are improving or not
# Build a table of false negatives and false positives for each value of k
# Draw a histogram of this table when done
# Use Cross-Tabulation in gmodels package to evaluate the result
# 5050 train/test set
CrossTable(x=test50.scaled.k3$cluster,y=bf5050.knn3,prop.chisq=FALSE)
CrossTable(x=test50.scaled.k5$cluster,y=bf5050.knn5,prop.chisq=FALSE)
CrossTable(x=test50.scaled.k7$cluster,y=bf5050.knn7,prop.chisq=FALSE)
# 6040 train/test set
CrossTable(x=test40.scaled.k3$cluster,y=bf6040.knn3,prop.chisq=FALSE)
CrossTable(x=test40.scaled.k5$cluster,y=bf6040.knn5,prop.chisq=FALSE)
CrossTable(x=test40.scaled.k7$cluster,y=bf6040.knn7,prop.chisq=FALSE)
# 7030 train/test set
CrossTable(x=test30.scaled.k3$cluster,y=bf7030.knn3,prop.chisq=FALSE)
CrossTable(x=test30.scaled.k5$cluster,y=bf7030.knn5,prop.chisq=FALSE)
CrossTable(x=test30.scaled.k7$cluster,y=bf7030.knn7,prop.chisq=FALSE)
test30.scaled.k7$cluster
test30.scaled.k7<- kmeans(test30,7)
test30.scaled.k7$cluster
bf7030.knn7 <- knn(train70.scaled,test30.scaled,train70.scaled.k3$cluster,k=7)
CrossTable(x=test30.scaled.k7$cluster,y=bf7030.knn7,prop.chisq=FALSE)
# combine two data sets into one
str(train50.scaled)
# combine two data sets into one
train50.scaled
# combine two data sets into one
train50
# combine two data sets into one
str(train50)
combined5050 <- rbind(train50,test50)
combined5050
combined6040 <- rbind(train60,test40)
combined7030 <- rbind(train70,test30)
combined5050
combined6040
combined7030
# Scale the combined set
combined5050.scaled <- as.data.frame(scale(combined5050))
# Scale the combined set
combined5050.scaled <- as.data.frame(scale(combined5050))
combined6040.scaled <- as.data.frame(scale(combined6040))
combined7030.scaled <- as.data.frame(scale(combined7040))
combined7030.scaled <- as.data.frame(scale(combined7030))
# Normalize the combined sets
normalize(combined5050.scaled)
# Normalize the combined sets
combined5050 <- normalize(combined5050.scaled)
combined6040 <- normalize(combined6040.scaled)
combined7030 <- normalize(combined7030.scaled)
# combine two data sets into one
train50
test50
combined5050 <- rbind(train50,test50)
combined5050
train60
test40
combined6040 <- rbind(train60,test40)
combined6040
train70
test30
combined7030 <- rbind(train70,test30)
combined7030
# Scale the combined set
combined5050.scaled <- as.data.frame(scale(combined5050))
combined6040.scaled <- as.data.frame(scale(combined6040))
combined7030.scaled <- as.data.frame(scale(combined7030))
# Normalize the combined sets
combined5050.scaled <- normalize(combined5050.scaled)
combined6040.scaled <- normalize(combined6040.scaled)
combined7030.scaled <- normalize(combined7030.scaled)
# lm method
# lm on Purchase vs. Age
combined5050.scaled$Purchase
combined5050.scaled$Age
lm(combined5050.scaled$Purchase ~ combined5050.scaled$Age, data = combined5050)
# lm method
# lm on Purchase vs. Age
# 5050 set
lm(combined5050.scaled$Purchase ~ combined5050.scaled$Age, data = combined5050)
# 6040 set
lm(combined6040.scaled$Purchase ~ combined6040.scaled$Age, data = combined6040)
# 7030 set
lm(combined7030.scaled$Purchase ~ combined7030.scaled$Age, data = combined7030)
# lm on Purchase vs. Stay_In_Current_City_Years
# 5050 set
lm(combined5050.scaled$Purchase ~ combined5050.scaled$Stay_In_Current_City_Years, data = combined5050)
# 6040 set
lm(combined6040.scaled$Purchase ~ combined6040.scaled$Stay_In_Current_City_Years, data = combined6040)
# 7030 set
lm(combined7030.scaled$Purchase ~ combined7030.scaled$Stay_In_Current_City_Years, data = combined7030)
# lm on Purchase vs. Occupation
# 5050 set
lm(combined5050.scaled$Purchase ~ combined5050.scaled$Stay_In_Current_City_Years, data = combined5050)
# 6040 set
lm(combined6040.scaled$Purchase ~ combined6040.scaled$Stay_In_Current_City_Years, data = combined6040)
# 7030 set
lm(combined7030.scaled$Purchase ~ combined7030.scaled$Stay_In_Current_City_Years, data = combined7030)
# lm method
# lm on Purchase vs. Age
# 5050 set
lm1 <- lm(combined5050.scaled$Purchase ~ combined5050.scaled$Age, data = combined5050)
plot(lm1$fitted.values,lm1$residuals)
plot(lm1$fitted.values,lm1$residuals)
# glm method (Same as above but with glm)
# glm on Purchase vs. Age
# 5050 set
glm1 <- glm(formula = combined5050$Purchase~combined5050$Age,family=gaussian,data=combined5050)
# glm method (Same as above but with glm)
# glm on Purchase vs. Age
# 5050 set
glm1 <- glm(formula = combined5050$Purchase~combined5050$Age,family=gaussian,data=combined5050)
glm1
# lm method
# lm on Purchase vs. Age
# Plot of fitted value versus residuals following every lm
# 5050 set
lm1 <- lm(combined5050.scaled$Purchase ~ combined5050.scaled$Age, data = combined5050.scaled)
plot(lm1$fitted.values,lm1$residuals)
plot(lm1$fitted.values,lm1$residuals)
# glm method (Same as above but with glm)
# glm on Purchase vs. Age
# 5050 set
glm1 <- glm(formula = combined5050.scaled$Purchase~combined5050$Age,family=gaussian,data=combined5050.scaled)
glm1
# 6040 set
glm2 <- glm(formula = combined6040.scaled$Purchase~combined6040$Age,family=gaussian,data=combined6040.scaled)
glm2
# 7030 set
glm3 <- glm(formula = combined7030$Purchase~combined7030$Age,family=gaussian,data=combined7030.scaled)
glm3
