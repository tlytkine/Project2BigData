rm(permits_accident_data_2013_Present)
ls()
ls
ls()
rm(n)
rm(train)
rm(z)
rm(sid)
rm(training)
rm(test)
rm(training_set)
rm(test_tree)
rm(treeAnalysis)
rm(testing)
rm(x)
rm(testing_set)
rm(y)
ls()
rm(character(0))
ls()
a = 49
sqrt(a)
a = "The dog ate my homework"
sub("dog","cat",a)
a = (1+1==3)
a
a = c(1,2,3)
a*2
a<-matrix(data=0,nr=1,nc=3)
a
doe = list(name="john",age=28,married=F)
doe$name
doe$age
a
addPercent <- function(x)
{
percent <-round(x*100,digits=1)
result <- paste(percent,"%",sep="")
return result
}
addPercent <- function(x)
{
percent <-round(x*100,digits=1)
result <- paste(percent,"%",sep="")
return(result)
}
install.packages("readr")
library(readr)
while(1){
print("Yo")
}
# take input from the user
num = as.integer(readline(prompt="Enter a number: "))
# check if the number is negative, positive
if (num < 0)
{
print("Sorry, factorial does not exist for negative numbers")
}
else if(num == 0)
{
print("The factorial of 0 is 1")
}
else
{
for(i in 1:num)
{
factorial = factorial * i
}
}
print(paste("The factorial of",num,"is",factorial))
}
# take input from the user
num = as.integer(readline(prompt="Enter a number: "))
factorial = 1
# check if the number is negative, positive
if (num < 0)
{
print("Sorry, factorial does not exist for negative numbers")
}
else if(num == 0)
install.packages("readr")
library(readr)
install.packages("dplyr")
install.packages("dplyr")
install.packages(ggplot2)
install.packages(ggplot)
install.packages("ggplot2")
installed.packages("esquisse")
test_expression = "yes"
if(test_expression){
print("Yes")
} else {
print("No")
}
test_expression = "yes"
if (test_expression){
print("Yes")
} else {
print("No")
}
test_expression = TRUE
if (test_expression){
print("Yes")
} else {
print("No")
}
test_expression = TRUE
if (test_expression){
print("Yes")
} else {
print("No")
}
install.packages("igraph")
install.packages("sna")
plot(a)
plot(doe)
library(data.table)
library(ggplot2)
library(gridExtra)
library(corrplot)
library(rpart)
library(randomForest)
library(stepPlr)
library(C50)
library(plyr)
library(MASS)
library(caret)
library(caretEnsemble)
library(dplyr)
library(plotly)
# make subplots
p <- subplot(
# histogram (counts) of gear
plot_ly(d, x = ~factor(gear)) %>%
add_histogram(color = I("grey50")),
# scatterplot of disp vs mpg
scatterplot,
titleX = T
)
# define a shared data object
d <- SharedData$new(mtcars)
g <- ggplot(txhousing, aes(x = date, y = sales, group = city)) +
geom_line(alpha = 0.4)
g
ggplotly(g, tooltip = c("city"))
g <- txhousing %>%
# group by city
group_by(city) %>%
# initiate a plotly object with date on x and median on y
plot_ly(x = ~date, y = ~median) %>%
# add a line plot for all texan cities
add_lines(name = "Texan Cities", hoverinfo = "none",
type = "scatter", mode = "lines",
line = list(color = 'rgba(192,192,192,0.4)')) %>%
# plot separate lines for Dallas and Houston
add_lines(name = "Houston",
data = filter(txhousing,
city %in% c("Dallas", "Houston")),
hoverinfo = "city",
line = list(color = c("red", "blue")),
color = ~city)
g
library(crosstalk)
# define a shared data object
d <- SharedData$new(mtcars)
# make a scatterplot of disp vs mpg
scatterplot <- plot_ly(d, x = ~mpg, y = ~disp) %>%
add_markers(color = I("navy"))
# define two subplots: boxplot and scatterplot
subplot(
# boxplot of disp
plot_ly(d, y = ~disp) %>%
add_boxplot(name = "overall",
color = I("navy")),
# scatterplot of disp vs mpg
scatterplot,
shareY = TRUE, titleX = T) %>%
layout(dragmode = "select")
# make subplots
p <- subplot(
# histogram (counts) of gear
plot_ly(d, x = ~factor(gear)) %>%
add_histogram(color = I("grey50")),
# scatterplot of disp vs mpg
scatterplot,
titleX = T
)
layout(p, barmode = "overlay")
library(networkD3)
data(MisLinks, MisNodes)
head(MisLinks, 3)
head(MisNodes, 3)
forceNetwork(Links = MisLinks, Nodes = MisNodes, Source = "source",
Target = "target", Value = "value", NodeID = "name",
Group = "group", opacity = 0.9, Nodesize = 3,
linkDistance = 100, fontSize = 20)
# Scatterplot
gg <- ggplot(midwest, aes(x=area, y=poptotal)) +
geom_point(aes(col=state, size=popdensity)) +
geom_smooth(method="loess", se=F) +
xlim(c(0, 0.1)) +
ylim(c(0, 500000)) +
labs(subtitle="Area Vs Population",
y="Population",
x="Area",
title="Scatterplot",
caption = "Source: midwest")
plot(gg)
install.packages("cowplot")  # a gganimate dependency
install.packages("devtools")
devtools::install_github("https://github.com/thomasp85/gganimate/releases/tag/v0.1.1")
install.packages("gapminder")
library(ggplot2)
library(gganimate)
library(gapminder)
theme_set(theme_bw())  # pre-set the bw theme.
g <- ggplot(gapminder, aes(gdpPercap, lifeExp, size = pop, frame = year)) +
geom_point() +
geom_smooth(aes(group = year),
method = "lm",
show.legend = FALSE) +
facet_wrap(~continent, scales = "free") +
scale_x_log10()  # convert to log scale
gganimate(g, interval=0.2)
# Load libraries
library(readxl)
library(dplyr)
library(psych)
library(ggplot2)
library(base)
library(data.table)
library(gridExtra)
library(corrplot)
library(rpart)
library(randomForest)
library(stepPlr)
library(C50)
library(plyr)
library(MASS)
library(caret)
library(caretEnsemble)
BlackFriday <- read.csv("BlackFriday.csv",stringsAsFactors = FALSE, header = TRUE, na.strings = c ('NA',''))
names(BlackFriday) <- c("User_ID","Product_ID","Gender","Age","Occupation","City_Category","Stay_In_Current_City_Years","Marital_Status","Product_Category_1","Product_Category_2","Product_Category_3","Purchase")
setwd("~/Desktop/Data Science/BigData /Project2BigData")
BlackFriday <- read.csv("BlackFriday.csv",stringsAsFactors = FALSE, header = TRUE, na.strings = c ('NA',''))
names(BlackFriday) <- c("User_ID","Product_ID","Gender","Age","Occupation","City_Category","Stay_In_Current_City_Years","Marital_Status","Product_Category_1","Product_Category_2","Product_Category_3","Purchase")
BlackFriday
# original data set
BlackFriday
# Get rid of na's
BlackFriday <- na.omit(BlackFriday)
# Summary
summary(BlackFriday)
# Describe
describe(BlackFriday)
BlackFriday
# Copy data to a working variable
BlackFridayClean <- BlackFriday
# Removing missing data
BlackFridayClean <- na.omit(BlackFridayClean)
# View BlackFridayClean
BlackFridayClean
colnames(BlackFridayClean)
bf <- BlackFridayClean
colnames(bf)
# Summary
summary(BlackFriday)
# Summary
summary(bf)
# Describe
describe(bf)
#  Calculate Distances (Classical methods for distance measures are Euclidean and Manhattan Distances)
#   - Select a distance measure: Euclidean, Manhattan, Minkowski, and so on
dist(bf[,1:10])
colnames(bf)
colnames(bf)
# Cluster Analysis
#
# Choose appropriate attributes
#   - Look for attributes that differentiate items in the set
#   - A sophisticated cluster analysis cannot compensate for a poor choice of variables
bf
# Make table with UserID, gender, age, city category, purchase
bf.data <- select(bf,User_ID,Gender,Age,City_Category,Purchase)
# Make table with UserID, gender, age, city category, purchase
bf.data <- dplyr::select(bf,User_ID,Gender,Age,City_Category,Purchase)
bf.data
psych:describe(bf.data)
bf.data <- na.omit(bf.data)
bf.data
psych::describe(bf.data,na.rm=TRUE)
describe(bf.data)
psych::describe(bf.data,na.rm=TRUE)
summary(bf.data)
str(bf.data)
# Scale the data
bf.data.scaled <- scale(bf.data)
bf.data.scaled
# Scale the data
bf.data.scaled <- scale(bf.data$Purchase)
bf.data.scaled
# Clusters for UserID
userids <- bf.data$User_ID
userids
userids <- na.omit(userids)
userids
uidClust <- kmeans(userids,centers=2,nstart=25)
uidClust
#  Calculate Distances (Classical methods for distance measures are Euclidean and Manhattan Distances)
#   - Select a distance measure: Euclidean, Manhattan, Minkowski, and so on
dist(bf[,1:10])
# Clusters for Gender
genders <- bf.data$Gender
genders
genders <- na.omit(genders)
genders
genderClust <- kmeans(userids,centers=2,nstart=25)
genderClust
uidClust
# Clusters for Age
ages <- bf.data$Age
age <- na.omit(age)
ages <- na.omit(ages)
ages
agesClust <- kmeans(ages,center=2,nstart=25)
agesClust <- kmeans(ages,center=3,nstart=25)
agesClust <- kmeans(ages,center=2,nstart=25)
agesClust <- kmeans(ages,center=2,nstart=1)
# Below doesn't work
# agesClust <- kmeans(ages,center=2,nstart=1)
iclust(ages,nclusters=4)
ages
# Below doesn't work
# agesClust <- kmeans(ages,center=2,nstart=1)
iclust(ages,nclusters=4)
# Below doesn't work
# agesClust <- kmeans(ages,center=2,nstart=1)
iclust(genders,nclusters=4)
# Below doesn't work
# agesClust <- kmeans(ages,center=2,nstart=1)
iclust(bf.data,nclusters=4)
str(bf.data)
bf$Gender <- as.numeric(bf$Gender)
bf$Gender <- na.omit(bf$Gender)
bf$Gender
# Make table with UserID, gender, age, city category, purchase
bf.data <- dplyr::select(bf,User_ID,Gender,Age,City_Category,Purchase)
bf.data <- na.omit(bf.data)
bf.data
str(bf.data)
BlackFriday <- read.csv("BlackFriday.csv",stringsAsFactors = FALSE, header = TRUE, na.strings = c ('NA',''))
names(BlackFriday) <- c("User_ID","Product_ID","Gender","Age","Occupation","City_Category","Stay_In_Current_City_Years","Marital_Status","Product_Category_1","Product_Category_2","Product_Category_3","Purchase")
BlackFriday
# Copy data to a working variable
BlackFridayClean <- BlackFriday
# Removing missing data
BlackFridayClean <- na.omit(BlackFridayClean)
# View BlackFridayClean
BlackFridayClean
colnames(BlackFridayClean)
colnames(BlackFridayClean)
bf <- BlackFridayClean
colnames(bf)
# Summary
summary(bf)
# Describe
describe(bf)
str(bf)
# Make table with UserID, gender, age, city category, purchase
bf.data <- dplyr::select(bf,User_ID,Gender,Age,City_Category,Purchase)
bf.data <- na.omit(bf.data)
bf.data
psych::describe(bf.data,na.rm=TRUE)
summary(bf.data)
str(bf.data)
userIDvsPurchase <- dplyr::select(bf,User_ID,Purchase)
userIDvsPurchase.scaled <- scale(userIDvsPurchase)
uidvsp <- userIDvsPurchase.scaled
str(uidvsp)
uidvsp <- na.omit(uidvsp)
uidvspClust <- kmeans(uidvsp,centers=2,nstart=25)
uidvspClust
iclust(udivsp,nclusters=4)
iclust(uidvsp,nclusters=4)
iclust(bf,nclusters=4)
iclust(bf.data)
sapply(bf,is.numeric)
bfn <- dplyr::select(bf,User_ID,Occupation,Product_Category_1,Product_Category_2,Product_Category_3,Purchase,Marital_Status)
bfn
bfn <- na.omit(bfn)
bfn
iclust(bfn)
# k-means clustering
k1 <- kmeans(bfn,centers=2,nstart = 25)
k1
# hclust
hc <- hclust(dist(bfn), "ave")
# hclust
dist(bfn)
# hclust
hc <- hclust(bfn, "ave")
# hclust
hc <- hclust(dist(bfn[,:100]), "ave")
# hclust
hc <- hclust(dist(bfn[,1:100]), "ave")
# hclust
hc <- hclust(dist(bfn[1:100,]), "ave")
plot(hc)
plot(hc, hang = -1)
# Scale the data
#   - If the variables vary in range, the variables with the largest values will predominate
#   and skew results
bfn.scaled <- scale(bfn)
normalize(bfn)
#   - Normalize (standardize) variables to a range [0..1] with standard deviation of 1.
#   - Other techniques may apply
# Min-Max Normalization
normalize <- function(x) {(((x-min(x))/max(x)-min(x)))}
normalize
normalize(bfn)
BlackFriday <- read.csv("BlackFriday.csv",stringsAsFactors = FALSE, header = TRUE, na.strings = c ('NA',''))
names(BlackFriday) <- c("User_ID","Product_ID","Gender","Age","Occupation","City_Category","Stay_In_Current_City_Years","Marital_Status","Product_Category_1","Product_Category_2","Product_Category_3","Purchase")
BlackFriday
# Copy data to a working variable
BlackFridayClean <- BlackFriday
# Removing missing data
BlackFridayClean <- na.omit(BlackFridayClean)
# View BlackFridayClean
BlackFridayClean
colnames(BlackFridayClean)
bf <- BlackFridayClean
colnames(bf)
# Summary
summary(bf)
# Describe
describe(bf)
# Structure of bf
str(bf)
# Cluster Analysis
# Choose appropriate attributes
#   - Look for attributes that differentiate items in the set
#   - A sophisticated cluster analysis cannot compensate for a poor choice of variables
#  Check which values are numeric
sapply(bf,is.numeric)
# User_ID, Occupation, Product Category 1, Product Category 2, Product Category 3, Purchase, Marital Status are numeric
# Make table of only numeric values
bfn <- dplyr::select(bf,User_ID,Occupation,Product_Category_1,Product_Category_2,Product_Category_3,Purchase,Marital_Status)
# Take out na's
bfn <- na.omit(bfn)
bfn
# k-means clustering
k1 <- kmeans(bfn,centers=2,nstart = 25)
k1
# iClust
iclust(bfn)
# hclust
hc <- hclust(dist(bfn[1:100,]), "ave")
plot(hc)
plot(hc, hang = -1)
# Scale the data
#   - If the variables vary in range, the variables with the largest values will predominate
#   and skew results
bfn.scaled <- scale(bfn)
#   - Normalize (standardize) variables to a range [0..1] with standard deviation of 1.
#   - Other techniques may apply
# Min-Max Normalization
normalize <- function(x) {(((x-min(x))/max(x)-min(x)))}
normalize(bfn.scaled)
#  Calculate Distances (Classical methods for distance measures are Euclidean and Manhattan Distances)
#   - Select a distance measure: Euclidean, Manhattan, Minkowski, and so on
dist(bfn)
#  Calculate Distances (Classical methods for distance measures are Euclidean and Manhattan Distances)
#   - Select a distance measure: Euclidean, Manhattan, Minkowski, and so on
dist(bfn[1:100,])
# k-means clustering
k1 <- kmeans(bfn.scaled,centers=2,nstart = 25)
k1
k1
plot(k1)
ggplot(k1)
# iClust
iclust(bfn.scaled)
# 50-50
train50 <- bf %>% dplyr::sample_frac(.50)
test50  <- dplyr::anti_join(bf, train, by = 'User_ID')
test50  <- dplyr::anti_join(bf, train, by = "User_ID")
# 50-50
train50 <- bf %>% dplyr::sample_frac(.50)
test50  <- dplyr::anti_join(bf, train50, by = "User_ID")
train50
str(train50)
str(test50)
# 50-50
bf$id <- 1:nrow(mtcars)
# 50-50
bf$id <- 1:nrow(bf)
train50 <- bf %>% dplyr::sample_frac(.50)
test50  <- dplyr::anti_join(bf, train50, by = 'id')
train50
str(train50)
str(test50)
# 60-40
train60 <- bf %>% dplyr::sample_frac(.60)
test40  <- dplyr::anti_join(bf, train60, by = 'id')
str(train60)
str(test40)
str(train60)
# 70-30
train70 <- bf %>% dplyr::sample_frac(.70)
test30  <- dplyr::anti_join(bf, train70, by = 'id')
str(train70)
str(test30)
